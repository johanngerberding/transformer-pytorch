{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "236353d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle \n",
    "from config import get_cfg_defaults\n",
    "from data import SOS_ID\n",
    "\n",
    "from transformer import subsequent_mask\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fccf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import encode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2960d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a340ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_root = \"/home/johann/sonstiges/transformer-pytorch/exps/2022-03-19_64_3_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01c275b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(os.path.join(exp_root, 'config.yaml'))\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ff2e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(exp_root, 'dataset.file'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64c4a80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.src_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16c89bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(41059, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(45359, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=45359, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = os.path.join(exp_root, \"en-de-model-iter-0000020000.pt\")\n",
    "model = torch.load(model_checkpoint)\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a29eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"hi my name is johann and i am a scientific researcher from germany who loves hacking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba2da311",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode_sentence(cfg, dataset, test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3047fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35856, 30995, 32578, 24216, 4980, 6980, 17841, 31700, 33771, 16056, 15923, 25109, 31785, 2769, 22645, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42caf206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35856, 30995, 32578, 24216,  4980,  6980, 17841, 31700, 33771, 16056,\n",
       "         15923, 25109, 31785,  2769, 22645,     1,     3,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = torch.tensor(encoded).unsqueeze(0).to(DEVICE)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4eeb756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_mask = (encoded != 0).unsqueeze(-2).to(DEVICE)\n",
    "encoded_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fceb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.encode(encoded, encoded_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "700c5e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = torch.ones(1,1).fill_(SOS_ID)\n",
    "tgt = tgt.long()\n",
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cda76283",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(99):\n",
    "        out = model.decode(\n",
    "            encoded, encoded_mask, \n",
    "            Variable(tgt), Variable(subsequent_mask(tgt.size(1)))\n",
    "        )\n",
    "        probs = model.generator(out[:, -1])\n",
    "        _, nextword = torch.max(probs, dim=1)\n",
    "        nextword = nextword.item()\n",
    "        tgt = torch.cat([\n",
    "            tgt, torch.ones(1,1).fill_(nextword).long()\n",
    "        ], dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9f03591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 35511, 35916, 30362, 11618, 33488, 10529, 27807, 25071, 7560, 1, 12780, 18759, 41514, 7122, 1, 28882, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "translation_ = tgt[0].detach().cpu().numpy().tolist()\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a623def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(idx2word, sentence: list) -> str:\n",
    "    sen_l = [idx2word[w] for w in sentence]\n",
    "    # don't remove <unk>\n",
    "    red_sen = []\n",
    "    for w in sen_l:\n",
    "        if w == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            if w not in ['<s>', '<pad>']:\n",
    "                red_sen.append(w)\n",
    "                \n",
    "    sen = ' '.join(red_sen)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8969c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mein name ist freiburg und ich bin ein wissenschaftlichen <unk> aus deutschland , der <unk> .\n"
     ]
    }
   ],
   "source": [
    "translation = decode_sentence(dataset.tgt_idx2word, translation_)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0c264d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 5\n",
    "attn_map = model.encoder.layers[0].self_attn.attn[0, head].data[:17, :17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8cd14f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c004550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f683675abb0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attn_map, cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9073d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
